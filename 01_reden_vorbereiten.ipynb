{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f5c5849-9cd7-4724-9b45-15381a249cbe",
      "metadata": {
        "id": "1f5c5849-9cd7-4724-9b45-15381a249cbe"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c400cb80-7c9d-4a6c-940c-f14c0f126482",
      "metadata": {
        "id": "c400cb80-7c9d-4a6c-940c-f14c0f126482"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import gensim\n",
        "import gensim.corpora as corpora\n",
        "from gensim.utils import simple_preprocess\n",
        "import collections\n",
        "from collections import Counter\n",
        "import itertools\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from tqdm import tqdm\n",
        "from datetime import date\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lemmatisieren und Stoppwörter entfernen\n",
        "1. Lemmatisieren des Textes\n",
        "2. Umlaute umwandeln\n",
        "3. Sonderzeichen und Bindestriche entfernen\n",
        "4. Standard Stoppwörter entfernen\n",
        "5. Text kleinschreiben\n",
        "\n",
        "**Dieser Abschnitt des Codes ist stark am Code von Latifi et al. (2024) orientiert. Dieser ist hier zu finden:** https://github.com/albi-lt/Fiscal-Policy-in-the-Bundestag/tree/main"
      ],
      "metadata": {
        "id": "6vsc0Q8zs71P"
      },
      "id": "6vsc0Q8zs71P"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6336d8f5-fad0-473b-83db-b1c502fc3c8a",
      "metadata": {
        "id": "6336d8f5-fad0-473b-83db-b1c502fc3c8a"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('german'))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Benötigte Funktionen."
      ],
      "metadata": {
        "id": "lIyeDDwrtQ7-"
      },
      "id": "lIyeDDwrtQ7-"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4aeccf83-bf7a-4bf5-aebb-e2402de8ea77",
      "metadata": {
        "id": "4aeccf83-bf7a-4bf5-aebb-e2402de8ea77"
      },
      "outputs": [],
      "source": [
        "def convert_umlauts(dataframe,textcolumn):\n",
        "    dataframe[textcolumn].replace('Ä','AE',regex=True, inplace = True)\n",
        "    dataframe[textcolumn].replace('ä','ae',regex=True, inplace = True)\n",
        "    dataframe[textcolumn].replace('Ü','UE',regex=True, inplace = True)\n",
        "    dataframe[textcolumn].replace('ü','ue',regex=True, inplace = True)\n",
        "    dataframe[textcolumn].replace('Ö','OE',regex=True, inplace = True)\n",
        "    dataframe[textcolumn].replace('ö','oe',regex=True, inplace = True)\n",
        "    dataframe[textcolumn].replace('ß','ss',regex=True, inplace = True)\n",
        "\n",
        "    return dataframe\n",
        "\n",
        "def convert_umlauts_strings(words):\n",
        "    mapping = {ord(u\"Ü\"): u\"Ue\", ord(u\"ü\"): u\"ue\", ord(u\"ß\"): u\"ss\", ord(u\"ä\"): u\"ae\", ord(u\"Ä\"): u\"Ae\",\n",
        "               ord(u\"ö\"): u\"oe\", ord(u\"Ö\"): u\"Oe\"}\n",
        "    # Übersetze jeden Eintrag in der Liste\n",
        "    converted_words = [word.translate(mapping) for word in words]\n",
        "\n",
        "    return converted_words\n",
        "\n",
        "def sent_to_words(sentences):\n",
        "    for sentence in sentences:\n",
        "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True,max_len = 30))\n",
        "        # Token mit nur einem oder mehr als 30 Zeichen werden exkludiert\n",
        "\n",
        "def remove_words(texts,words):\n",
        "    docs = []\n",
        "    for doc in tqdm(texts):\n",
        "        doc_cleaned = list(filter(lambda word: word.lower() not in words, doc.split()))\n",
        "        docs.append(' '.join(doc_cleaned))\n",
        "    return docs\n",
        "\n",
        "def remove_stopwords(texttoken,stopwordslist):\n",
        "    filtered_text = [word for word in texttoken if word.lower() not in stopwordslist]\n",
        "    return filtered_text\n",
        "\n",
        "def lemmatize_texts(texts, spacy_model):\n",
        "    nlp_model = spacy.load(spacy_model)\n",
        "    pp = []\n",
        "    print('Start lemmatization...')\n",
        "    t0 = time.time()\n",
        "    for i in tqdm(range(len(texts))):\n",
        "        text = \" \".join([token.lemma_ for token in nlp_model(texts[i])])#\n",
        "        pp.append(text)\n",
        "    t1 = time.time()\n",
        "    print('Finished lemmatization. Process took', t1 - t0, 'seconds')\n",
        "    return pp\n",
        "\n",
        "def remove_specific_stopwords(text):\n",
        "    words = text.split()\n",
        "    filtered_words = [word for word in words if word not in lemmatized_stopwords]\n",
        "    return ' '.join(filtered_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22bb282a-be7e-4a66-88e0-227c25335101",
      "metadata": {
        "collapsed": true,
        "id": "22bb282a-be7e-4a66-88e0-227c25335101"
      },
      "outputs": [],
      "source": [
        "data_all_lp = pd.read_csv('all_bundestag_speeches_replication_data.csv', index_col = 'Unnamed: 0')\n",
        "\n",
        "'''\n",
        "Die Reden sind ebenfalls auf dem GitHub von Latifi zu finden.\n",
        "'''\n",
        "\n",
        "data_all_lp.Role.unique()\n",
        "\n",
        "# Limitieren der Daten auf den Zeitraum ab 2000\n",
        "data_all_lp['Datum'] = pd.to_datetime(data_all_lp['date'], format='%d.%m.%Y')\n",
        "data_all_lp = data_all_lp.loc[data_all_lp['Datum'].dt.year >= 2000]\n",
        "\n",
        "# Filtern der Reden. Behalten werden: 'MdB','Bundeskanzler','Bundesminister','Staatssekretär', 'Staatsminister'\n",
        "data_filtered = data_all_lp[data_all_lp.Role.isin(['MdB','Bundesminister','Bundeskanzler','Staatssekretär','Staatsminister'])]\n",
        "\n",
        "print(len(data_filtered))\n",
        "data_filtered"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a84e0784-8b65-4c2c-b49d-c40320ea26c9",
      "metadata": {
        "collapsed": true,
        "id": "a84e0784-8b65-4c2c-b49d-c40320ea26c9"
      },
      "outputs": [],
      "source": [
        "pd.DataFrame(data_filtered.text_length).describe(percentiles = [0.05,0.1,0.15,0.25,0.5,0.75,0.8,0.9,0.95,0.99,0.995])\n",
        "\n",
        "# Zu lange und zu kurze Reden werden entfernt\n",
        "data_filtered = data_filtered[(data_filtered.text_length >= 100) & (data_filtered.text_length <= np.quantile(data_filtered.text_length,0.995))].reset_index(drop=True)\n",
        "print(len(data_filtered))\n",
        "data_filtered"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lemmatisieren"
      ],
      "metadata": {
        "id": "nsSaT3sivgIq"
      },
      "id": "nsSaT3sivgIq"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c146b770-0d96-42c4-8ee1-add25c47ade9",
      "metadata": {
        "id": "c146b770-0d96-42c4-8ee1-add25c47ade9"
      },
      "outputs": [],
      "source": [
        "# Lemmatisieren der Texte\n",
        "lemmatized = lemmatize_texts([data_filtered.text[i] for i in range(data_filtered.shape[0])], 'de_core_news_lg')\n",
        "\n",
        "# Erstellen einer neuen Spalte mit den lemmatisierten Texten\n",
        "data_filtered['lemmatized_text'] = lemmatized\n",
        "\n",
        "# Ersetzen von Zeilenumbrüchen innerhalb von Wörtern\n",
        "data_lemmatized = data_filtered.replace('-\\n', '', regex=True)\n",
        "\n",
        "# Speichern der Daten als CSV-Datei mit der neuen Spalte\n",
        "data_lemmatized.to_csv('data_lemmatized.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1e1c8d9-70c4-41fe-8f63-53183f8dca4a",
      "metadata": {
        "collapsed": true,
        "id": "c1e1c8d9-70c4-41fe-8f63-53183f8dca4a"
      },
      "outputs": [],
      "source": [
        "# Ersetzen von Zeilenumbrüchen in der neuen Spalte 'lemmatized_text'\n",
        "data_lemmatized['lemmatized_text'] = data_filtered['lemmatized_text'].replace('-\\n', '', regex=True)\n",
        "\n",
        "# Umwandeln von Umlauten in alternative Schreibweise\n",
        "data_lemmatized['lemmatized_text'] = convert_umlauts_strings(data_lemmatized['lemmatized_text'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ersetze Zeilenumbrüche in der neuen Spalte 'lemmatized_text'\n",
        "data_lemmatized['lemmatized_text'] = data_lemmatized['lemmatized_text'].replace('-\\n', '', regex=True)"
      ],
      "metadata": {
        "id": "HvjTmo-9MwJo"
      },
      "id": "HvjTmo-9MwJo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Entfernen von nltk-Stoppwörtern"
      ],
      "metadata": {
        "id": "BzR2YFJnvccD"
      },
      "id": "BzR2YFJnvccD"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bdd18716-14bf-4b14-b61c-8e2c3372402f",
      "metadata": {
        "id": "bdd18716-14bf-4b14-b61c-8e2c3372402f"
      },
      "outputs": [],
      "source": [
        "# Entfernen von Standard Stoppwörtern\n",
        "data_lemmatized['text_preprocessed'] = data_lemmatized['text_preprocessed'].apply(\n",
        "    lambda words: [word for word in words if word.lower() not in stop_words]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Funktion zum Entfernen der Leerzeichen zwischen Buchstaben\n",
        "def remove_spaces_in_words(word_list):\n",
        "    # Entfernen von Leerzeichen aus jedem \"Wort\"\n",
        "    return [\"\".join(word.split()) for word in word_list]\n",
        "\n",
        "# Anwenden der Funktion auf die Spalte 'text_preprocessed'\n",
        "data_lemmatized['text_preprocessed'] = data_lemmatized['text_preprocessed'].apply(remove_spaces_in_words)"
      ],
      "metadata": {
        "id": "icg3y4Vjdax3"
      },
      "id": "icg3y4Vjdax3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Umbenennen von Spaltennamen\n",
        "data_lemmatized.rename(columns={'period': 'periode'}, inplace=True)\n",
        "data_lemmatized.rename(columns={'governing_Party': 'regierung'}, inplace=True)\n",
        "data_lemmatized.rename(columns={'Party': 'partei'}, inplace=True)\n",
        "data_lemmatized.rename(columns={'date': 'Datum'}, inplace=True)\n",
        "data_lemmatized.rename(columns={'text_length': 'länge_vorher'}, inplace=True)"
      ],
      "metadata": {
        "id": "SI1ephwpFfki"
      },
      "id": "SI1ephwpFfki",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba25ef86-15c4-4cf0-9ed2-c8eca3c6ad6d",
      "metadata": {
        "id": "ba25ef86-15c4-4cf0-9ed2-c8eca3c6ad6d"
      },
      "outputs": [],
      "source": [
        "# Vorverarbeitete Reden in einer neuen .csv speichern\n",
        "data_lemmatized[['doc_id', 'doc_lp_id', 'speech_identification_ent', 'Datum', 'period', 'governing_Party', 'text_preprocessed', 'Party']].to_csv(\n",
        "    'data_preprocessed.csv',\n",
        "    index=False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b742ffbe-4da7-4a2e-9c7d-fd8621a1dffe",
      "metadata": {
        "id": "b742ffbe-4da7-4a2e-9c7d-fd8621a1dffe"
      },
      "outputs": [],
      "source": [
        "text_length_preprocessed = [len(i.split()) for i in data_lemmatized.text_preprocessed] #counts document lengths after removing stopwords\n",
        "# text_length_lemmatized = [len(i.split()) for i in data_lemmatized.text_preprocessed_lemmatized]\n",
        "\n",
        "data_lemmatized['länge_bereinigt'] = text_length_preprocessed\n",
        "# data_lemmatized['text_length_lemmatized'] = text_length_lemmatized\n",
        "\n",
        "data_lemmatized['datum_jahr'] = data_lemmatized.date.dt.year\n",
        "data_lemmatized['datum_quartal'] = data_lemmatized.date.dt.quarter\n",
        "\n",
        "data_lemmatized[['doc_id', 'doc_lp_id', 'speech_identification_ent', 'Datum', 'datum_jahr', 'datum_quartal', 'period', 'governing_Party', 'text_preprocessed', 'länge_vorher', 'länge_bereinigt', 'Party']].to_csv(\n",
        "    'speeches_preprocessed.csv',\n",
        "    index=False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "speeches_preprocessed.rename(columns={'period': 'periode'}, inplace=True)\n",
        "speeches_preprocessed.rename(columns={'date_year': 'datum_jahr'}, inplace=True)\n",
        "speeches_preprocessed.rename(columns={'date_quarter': 'datum_quartal'}, inplace=True)\n",
        "speeches_preprocessed.rename(columns={'governing_Party': 'regierung'}, inplace=True)\n",
        "speeches_preprocessed.rename(columns={'Party': 'partei'}, inplace=True)\n",
        "speeches_preprocessed.rename(columns={'date': 'Datum'}, inplace=True)\n",
        "speeches_preprocessed.rename(columns={'text_length': 'länge_vorher'}, inplace=True)\n",
        "speeches_preprocessed.rename(columns={'text_length_preprocessed': 'länge_bereinigt'}, inplace=True)"
      ],
      "metadata": {
        "id": "i2lPe7MYtrr2"
      },
      "id": "i2lPe7MYtrr2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc7eb57b-6288-450e-8ae5-d26139a52968",
      "metadata": {
        "id": "cc7eb57b-6288-450e-8ae5-d26139a52968"
      },
      "outputs": [],
      "source": [
        "länge_original = speeches_preprocessed['länge_vorher'].sum()\n",
        "länge_preprocessed = speeches_preprocessed['länge_bereinigt'].sum()\n",
        "\n",
        "entfernte_wörter = länge_original - länge_preprocessed\n",
        "\n",
        "print(f\"Vor dem Entfernen allgemeiner Stoppwörter umfasste der Datensatz {länge_original} Wörter.\")\n",
        "print(f\"Durch das Entfernen entfielen {entfernte_wörter} Wörter.\")\n",
        "print(f\"Somit umfasst der Korpus jetzt noch {länge_preprocessed} Wörter.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa7c1169-dda6-4caf-9f00-e035141409ae",
      "metadata": {
        "id": "aa7c1169-dda6-4caf-9f00-e035141409ae"
      },
      "source": [
        "### Entfernen spezifischer Stoppwörter\n",
        "\n",
        "Die Stoppwörter basieren auf der Liste von Latifi et al., (2024) und wurden manuell angepasst."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dcb33787-543d-4c22-96a6-14513cb23269",
      "metadata": {
        "id": "dcb33787-543d-4c22-96a6-14513cb23269"
      },
      "outputs": [],
      "source": [
        "# Schritt 1: Einlesen der CSV mit spezifischen Stoppwörtern\n",
        "stopwords_df = pd.read_csv('self_filtered_stopwords_german_bundestag.csv', header=None)\n",
        "\n",
        "# Schritt 2: Extrahieren der lemmatisierten Stoppwörter aus der Tabelle\n",
        "lemmatized_stopwords = stopwords_df[2].drop(0).tolist()\n",
        "\n",
        "# Anwenden der Funktion auf die Spalte 'text_preprocessed'\n",
        "speeches_preprocessed['text_preprocessed_specific'] = speeches_preprocessed['text_preprocessed'].apply(remove_specific_stopwords)\n",
        "\n",
        "# Wörter nach der Entfernung spezifischer Stoppwörter zählen\n",
        "länge_specific = speeches_preprocessed['text_preprocessed_specific'].apply(lambda x: len(str(x).split())).sum()\n",
        "\n",
        "# Schritt 3: Ergebnisse ausgeben\n",
        "entfernte_wörter_specific = länge_preprocessed - länge_specific\n",
        "print(f\"Vor dem Entfernen spezifischer Stoppwörter umfasste der Datensatz {länge_preprocessed} Wörter.\")\n",
        "print(f\"Durch das Entfernen entfielen {entfernte_wörter_specific} Wörter.\")\n",
        "print(f\"Somit umfasst der Korpus jetzt noch {länge_specific} Wörter.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a14dbd0e-4ddf-41a8-bdcc-48b348f37f9c",
      "metadata": {
        "id": "a14dbd0e-4ddf-41a8-bdcc-48b348f37f9c"
      },
      "outputs": [],
      "source": [
        "text_length_specific = [len(i.split()) for i in speeches_preprocessed.text_preprocessed_specific] # Textlänge nach dem Bereinigen der Artikel\n",
        "\n",
        "speeches_preprocessed['länge_bereinigt_spezifisch'] = text_length_specific\n",
        "\n",
        "# Speichern in einer neuen .csv Datei\n",
        "speeches_preprocessed.to_csv('bereinigte_reden_spezifisch.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b98fa56c-2937-47c9-847e-c400cbe23054",
      "metadata": {
        "id": "b98fa56c-2937-47c9-847e-c400cbe23054"
      },
      "source": [
        "### Graphische Darstellung"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b02d634f-6f87-43cb-ab2e-e61c19b8c1be",
      "metadata": {
        "id": "b02d634f-6f87-43cb-ab2e-e61c19b8c1be"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31a139ed-6829-466a-b300-4949c2fb57ee",
      "metadata": {
        "id": "31a139ed-6829-466a-b300-4949c2fb57ee"
      },
      "outputs": [],
      "source": [
        "# Definieren von Bins für Gruppen von 20 Tokens\n",
        "bin_edges = range(0, speeches_preprocessed[\"länge_vorher\"].max() + 20, 20)\n",
        "\n",
        "# Frequenzberechnung für jede Textlängenspalte\n",
        "freq_original = pd.cut(speeches_preprocessed[\"länge_vorher\"], bins=bin_edges).value_counts().sort_index()\n",
        "freq_nltk = pd.cut(speeches_preprocessed[\"länge_bereinigt\"], bins=bin_edges).value_counts().sort_index()\n",
        "freq_specific = pd.cut(speeches_preprocessed[\"länge_bereinigt_spezifisch\"], bins=bin_edges).value_counts().sort_index()\n",
        "\n",
        "# Mittelpunkte der Bins\n",
        "bin_midpoints = [interval.mid for interval in freq_original.index.categories]\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.bar(bin_midpoints, freq_original, width=18, label=\"Originale Länge der Reden\", alpha=0.7, color='blue')\n",
        "plt.bar(bin_midpoints, freq_nltk, width=18, label=\"Länge nltk-Stoppwörter entfernt\", alpha=0.7, color='orange')\n",
        "plt.bar(bin_midpoints, freq_specific, width=18, label=\"Länge spezifische Stoppwörter entfernt\", alpha=0.7, color='green')\n",
        "\n",
        "# Achsenbeschriftungen und Titel\n",
        "plt.xlabel(\"Anzahl der Wörter in der Rede\", fontsize=12)\n",
        "plt.ylabel(\"Anzahl der Reden\", fontsize=12)\n",
        "plt.title(\"\", fontsize=14)\n",
        "plt.legend()\n",
        "plt.grid(alpha=0.3)\n",
        "plt.tight_layout()\n",
        "\n",
        "# Speichern der Grafik\n",
        "plt.savefig('EDA_reden_1.png', format='png', dpi=600)\n",
        "\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python (ba_env)",
      "language": "python",
      "name": "ba_env"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.10"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}