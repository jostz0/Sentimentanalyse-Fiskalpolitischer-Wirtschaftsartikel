{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install PyPDF2\n",
        "!pip install spacy\n",
        "!python -m spacy download de_core_news_lg"
      ],
      "metadata": {
        "collapsed": true,
        "id": "GcwHIHneVfvU"
      },
      "id": "GcwHIHneVfvU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "72c4ba0b-6351-42c5-9b81-5f5ec5f20fc3",
      "metadata": {
        "id": "72c4ba0b-6351-42c5-9b81-5f5ec5f20fc3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import spacy\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "import gensim\n",
        "import gensim.corpora as corpora\n",
        "from gensim.utils import simple_preprocess\n",
        "from nltk.corpus import stopwords\n",
        "from tqdm import tqdm\n",
        "from datetime import date\n",
        "import time\n",
        "import itertools\n",
        "import collections\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "98de899e-7872-4bf7-9127-b8ad1757fdb8",
      "metadata": {
        "id": "98de899e-7872-4bf7-9127-b8ad1757fdb8"
      },
      "outputs": [],
      "source": [
        "from PyPDF2 import PdfReader"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4669a811-5709-456e-84e4-d69bbe6f70ed",
      "metadata": {
        "id": "4669a811-5709-456e-84e4-d69bbe6f70ed"
      },
      "source": [
        "## Artikel in geordnete Form bringen"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Text aus PDFs extrahieren"
      ],
      "metadata": {
        "id": "kn6nc5gvitKr"
      },
      "id": "kn6nc5gvitKr"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd80a1ec-ae47-448e-874d-9d03e356d919",
      "metadata": {
        "id": "cd80a1ec-ae47-448e-874d-9d03e356d919"
      },
      "outputs": [],
      "source": [
        "# Funktion zum Lesen von Daten in einer .txt-Datei\n",
        "def load_data(file):\n",
        "    with open(file, \"r\", encoding=\"utf-8\") as f:\n",
        "        text = f.read()\n",
        "    return text\n",
        "\n",
        "# Funktion zum Schreiben von Daten in eine .txt-Datei\n",
        "def write_data(file, text):\n",
        "    with open(file, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "075ca5cb-d1c7-4205-ba8d-ecddce2bc3ac",
      "metadata": {
        "id": "075ca5cb-d1c7-4205-ba8d-ecddce2bc3ac"
      },
      "outputs": [],
      "source": [
        "# Schritt 1: PDF-Dateien öffnen und gesamten Text extrahieren\n",
        "text = \"\"\n",
        "\n",
        "pdf_directory = r\"D:\\Datensatz\\Handelsblatt\"  # Auf Verzeichnis anpassen\n",
        "\n",
        "# Durchlaufen aller PDF-Dateien (durchnummeriert von Dokument1.pdf bis Dokument36.pdf)\n",
        "for i in range(1, 37):\n",
        "    pdf_filename = f\"Dokument{i}.pdf\"\n",
        "    pdf_path = os.path.join(pdf_directory, pdf_filename)\n",
        "\n",
        "    # Prüfe, ob die Datei existiert\n",
        "    if os.path.exists(pdf_path):\n",
        "        print(f\"Verarbeite {pdf_filename}...\")\n",
        "        with open(pdf_path, \"rb\") as pdf_file:\n",
        "            reader = PdfReader(pdf_file)\n",
        "            # Extrahiere Text aus allen Seiten\n",
        "            for page in reader.pages:\n",
        "                text += page.extract_text() + \"\\n\"\n",
        "    else:\n",
        "        print(f\"Warnung: {pdf_filename} wurde nicht gefunden.\")\n",
        "\n",
        "# Speichern des gesamten Texts in einer .txt-Datei\n",
        "output_file = \"extrahierter_text.txt\"\n",
        "write_data(output_file, text)\n",
        "print(f\"Text aus allen PDFs wurde in {output_file} gespeichert.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Anpassen der Artikel"
      ],
      "metadata": {
        "id": "tNBMYevGiw28"
      },
      "id": "tNBMYevGiw28"
    },
    {
      "cell_type": "markdown",
      "id": "e0fd85c0-dd9a-4ac0-9e5f-63363f1245c9",
      "metadata": {
        "id": "e0fd85c0-dd9a-4ac0-9e5f-63363f1245c9"
      },
      "source": [
        "Anpassen des Datumsformats, sodass es für alle Artikel identisch ist."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e7ca5bc-0dc1-499b-b476-bd69eb8d1623",
      "metadata": {
        "id": "3e7ca5bc-0dc1-499b-b476-bd69eb8d1623"
      },
      "outputs": [],
      "source": [
        "text = load_data('extrahierter_text.txt')\n",
        "\n",
        "# Schritt 2: Datumsformat ändern (zweistellige Jahreszahlen in vierstellige umwandeln)\n",
        "def convert_short_year(match):\n",
        "    day, month, year = match.groups()\n",
        "    year = '20' + year if int(year) <= 50 else '19' + year\n",
        "    return f\"{day}.{month}.{year}\"\n",
        "\n",
        "pattern = r\"(\\d{2})\\.(\\d{2})\\.(\\d{2})(?=\\s|[^0-9])\"\n",
        "text = re.sub(pattern, convert_short_year, text)  # `text` wird aktualisiert"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unnötige Textteile entfernen."
      ],
      "metadata": {
        "id": "P2YrnDvwi1gm"
      },
      "id": "P2YrnDvwi1gm"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a8addbb-a44b-4703-b92b-5a84b91665fb",
      "metadata": {
        "id": "7a8addbb-a44b-4703-b92b-5a84b91665fb"
      },
      "outputs": [],
      "source": [
        "print(f'Der Term \"Handelsblatt print:\" kommt vor dem Bereinigen {text.count(\"Handelsblatt print:\")} mal vor.')\n",
        "\n",
        "# Schritt 3: Entfernen des Texts zwischen den alternativen Quellenangaben und dem nächsten \"Handelsblatt print:\" oder bis zum Ende des Dokuments\n",
        "pattern = r\"(Quelle Handelsblatt print:|Quelle Verlagsbeilage im Handelsblatt print:).*?(?=Handelsblatt print:|$)\"\n",
        "text = re.sub(pattern, \"\", text, flags=re.DOTALL)\n",
        "\n",
        "# Schritt 4: Entfernen von allem zwischen \"Handelsblatt print:\" und \"vom\", aber behalte \"Handelsblatt print:\" und \"vom\" selbst bei\n",
        "pattern = r\"(Handelsblatt print:).*?vom\"\n",
        "text = re.sub(pattern, r\"\\1 vom\", text, flags=re.DOTALL)\n",
        "\n",
        "# Schritt 5: Entfernen von allem von \"Seite  von\" bis zum nächsten Buchstaben\n",
        "pattern = r\"Seite  von [0-9\\s]+(?=[a-zA-ZäöüÄÖÜß])\"\n",
        "text = re.sub(pattern, \"\", text, flags=re.DOTALL)\n",
        "\n",
        "# Schritt 6: Entfernen von groß geschriebenen Wörtern, die länger als 4 Zeichen sind\n",
        "def remove_uppercase_words(text):\n",
        "    pattern = r'\\b[A-ZÄÖÜ]{5,}\\b'\n",
        "    text = re.sub(pattern, '', text)\n",
        "    return text\n",
        "\n",
        "# Funktion zur Bereinigung des Textes\n",
        "def clean_text_spacing(text):\n",
        "    # Entferne überflüssige Zeilenumbrüche oder doppelte Leerzeichen\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    return text\n",
        "\n",
        "text = remove_uppercase_words(text)\n",
        "text = clean_text_spacing(text)\n",
        "\n",
        "# Zählen des Vorkommen von \"Handelsblatt print:\" nach der Bereinigung (Kontrolle)\n",
        "occurrences = text.count(\"Handelsblatt print:\")\n",
        "print(f'Der Begriff \"Handelsblatt print:\" kommt {occurrences} Mal im bereinigten Text vor.')\n",
        "\n",
        "# Speichern des geänderten Texts wieder in einer .txt Datei\n",
        "write_data('bereinigter_text.txt', text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "78047278-731d-4c36-bd19-d40b40752af9",
      "metadata": {
        "id": "78047278-731d-4c36-bd19-d40b40752af9"
      },
      "source": [
        "### Speichern in einer .csv Datei"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import string"
      ],
      "metadata": {
        "id": "5-RDCu26ZWSd"
      },
      "id": "5-RDCu26ZWSd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bccbff18-cef6-4a7f-bec8-056698210fa2",
      "metadata": {
        "id": "bccbff18-cef6-4a7f-bec8-056698210fa2"
      },
      "outputs": [],
      "source": [
        "text = load_data('bereinigter_text.txt')\n",
        "\n",
        "# Schritt 1: Artikel und Daten extrahieren\n",
        "articles = re.split(r\"Handelsblatt print: vom \", text)[1:]  # Splitte anhand des Keywords und ignoriere den ersten Teil\n",
        "data = []\n",
        "\n",
        "def remove_punctuation(text):\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    return text\n",
        "\n",
        "# Schritt 2: Durch Artikel interieren und Datum und Text extrahieren\n",
        "for article in articles:\n",
        "    # Extrahieren des Datums (ersten 10 Zeichen)\n",
        "    date = article[:10]\n",
        "    if re.match(r\"\\d{2}\\.\\d{2}\\.\\d{4}\", date):\n",
        "        # Extrahieren des restlichen Texts\n",
        "        text = article[10:].strip()\n",
        "        # Entfernen des Teils vor dem nächsten \"Handelsblatt print:\" oder bis zum Ende\n",
        "        text = re.split(r\"Handelsblatt print: vom \\d{2}\\.\\d{2}\\.\\d{4}\", text, maxsplit=1)[0].strip()\n",
        "        # Datum hinzufügen\n",
        "        data.append({\"Datum\": date, \"Text\": text})\n",
        "\n",
        "for article_data in data:\n",
        "    article_data['Text'] = remove_punctuation(article_data['Text'])\n",
        "\n",
        "# Schritt 3: DataFrame erstellen\n",
        "df = pd.DataFrame(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Entfernen von doppelten Artikeln."
      ],
      "metadata": {
        "id": "-22TxDD3dCbe"
      },
      "id": "-22TxDD3dCbe"
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_duplicate_articles(df):\n",
        "    # Erstellen einer Hilfsspalte für den Vergleich (Kombination aus Datum und ersten 100 Zeichen)\n",
        "    df['duplicate_key'] = df['Datum'].astype(str) + '_' + df['Text'].str[:100]\n",
        "\n",
        "    # Entfernen von Duplikaten basierend auf der Hilfsspalte\n",
        "    df = df[~df.duplicated(subset='duplicate_key')].copy()\n",
        "\n",
        "    # Entfernen der Hilfsspalte\n",
        "    df.drop(columns=['duplicate_key'], inplace=True)\n",
        "\n",
        "    # Index zurücksetzen\n",
        "    df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "    return df\n",
        "\n",
        "df = remove_duplicate_articles(df)"
      ],
      "metadata": {
        "id": "v_7Ixp0-dGhn"
      },
      "id": "v_7Ixp0-dGhn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(df))"
      ],
      "metadata": {
        "id": "u7iEUd7qdTrm"
      },
      "id": "u7iEUd7qdTrm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Begrenzen auf Artikel mit mehr als 200 Wörtern."
      ],
      "metadata": {
        "id": "d5Ei6fpssDRx"
      },
      "id": "d5Ei6fpssDRx"
    },
    {
      "cell_type": "code",
      "source": [
        "# Funktion zur Zählung der Wörter in einem Artikel\n",
        "def word_count(text):\n",
        "    return len(str(text).split())\n",
        "\n",
        "# Zählen der Wörter und filtern der Artikel mit mindestens 200 Wörtern\n",
        "df['Word Count'] = df['Text'].apply(word_count)\n",
        "df_filtered = df[df['Word Count'] >= 200].drop(columns=['Word Count'])  # Entfernen der Hilfsspalte 'Word Count'\n",
        "\n",
        "print(len(df_filtered))"
      ],
      "metadata": {
        "id": "MF8f5EpVtvxM",
        "collapsed": true
      },
      "id": "MF8f5EpVtvxM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "DataFrame als .csv abspeichern."
      ],
      "metadata": {
        "id": "3zaXB_nkj23C"
      },
      "id": "3zaXB_nkj23C"
    },
    {
      "cell_type": "code",
      "source": [
        "df_filtered.to_csv('artikel_tabelle.csv', index=False, encoding=\"utf-8\")"
      ],
      "metadata": {
        "id": "RrorudxZuCI8"
      },
      "id": "RrorudxZuCI8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "c83b0d84-c7a8-4976-ac10-d295bee1be10",
      "metadata": {
        "id": "c83b0d84-c7a8-4976-ac10-d295bee1be10"
      },
      "source": [
        "## Lemmatisieren und Stoppwörter entfernen\n",
        "1. Lemmatisieren des Textes\n",
        "2. Umlaute umwandeln\n",
        "3. Sonderzeichen und Bindestriche entfernen\n",
        "4. Standard Stoppwörter entfernen\n",
        "5. Text kleinschreiben\n",
        "\n",
        "**Dieser Abschnitt des Codes ist stark am Code von Latifi et al. (2024) orientiert. Dieser ist hier zu finden:**\n",
        "https://github.com/albi-lt/Fiscal-Policy-in-the-Bundestag/tree/main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e8449881-cbd5-45fa-bff7-9100224722f5",
      "metadata": {
        "id": "e8449881-cbd5-45fa-bff7-9100224722f5",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('german'))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Benötigte Funktionen."
      ],
      "metadata": {
        "id": "tG4WoVyZkbU-"
      },
      "id": "tG4WoVyZkbU-"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42f8144b-823c-420f-9881-949b56cd5ccd",
      "metadata": {
        "id": "42f8144b-823c-420f-9881-949b56cd5ccd"
      },
      "outputs": [],
      "source": [
        "def convert_umlauts(dataframe,textcolumn):\n",
        "    dataframe[textcolumn].replace('Ä','AE',regex=True, inplace = True)\n",
        "    dataframe[textcolumn].replace('ä','ae',regex=True, inplace = True)\n",
        "    dataframe[textcolumn].replace('Ü','UE',regex=True, inplace = True)\n",
        "    dataframe[textcolumn].replace('ü','ue',regex=True, inplace = True)\n",
        "    dataframe[textcolumn].replace('Ö','OE',regex=True, inplace = True)\n",
        "    dataframe[textcolumn].replace('ö','oe',regex=True, inplace = True)\n",
        "    dataframe[textcolumn].replace('ß','ss',regex=True, inplace = True)\n",
        "\n",
        "    return dataframe\n",
        "\n",
        "def convert_umlauts_strings(texts):\n",
        "    mapping = {ord(u\"Ü\"): u\"Ue\", ord(u\"ü\"): u\"ue\", ord(u\"ß\"): u\"ss\", ord(u\"ä\"): u\"ae\", ord(u\"Ä\"): u\"Ae\",\n",
        "               ord(u\"ö\"): u\"oe\", ord(u\"Ö\"): u\"Oe\"}\n",
        "    converted_texts = [i.translate(mapping) for i in texts]\n",
        "\n",
        "    return converted_texts\n",
        "\n",
        "def sent_to_words(sentences):\n",
        "    for sentence in sentences:\n",
        "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True,max_len = 30))\n",
        "        #exclude tokens with one character and those with more than 30 characters\n",
        "\n",
        "def remove_words(texts,words):\n",
        "    docs = []\n",
        "    for doc in tqdm(texts):\n",
        "        doc_cleaned = list(filter(lambda word: word.lower() not in words, doc.split()))\n",
        "        docs.append(' '.join(doc_cleaned))\n",
        "    return docs\n",
        "\n",
        "def remove_stopwords(texttoken,stopwordslist):\n",
        "    filtered_text = [word for word in texttoken if word.lower() not in stopwordslist]\n",
        "    return filtered_text\n",
        "\n",
        "def lemmatize_texts(texts, spacy_model):\n",
        "    nlp_model = spacy.load(spacy_model)\n",
        "    pp = []\n",
        "    print('Start lemmatization...')\n",
        "    t0 = time.time()\n",
        "    for i in tqdm(range(len(texts))):\n",
        "        text = \" \".join([token.lemma_ for token in nlp_model(texts[i])])#\n",
        "        pp.append(text)\n",
        "    t1 = time.time()\n",
        "    print('Finished lemmatization. Process took', t1 - t0, 'seconds')\n",
        "    return pp\n",
        "\n",
        "def remove_specific_stopwords(text):\n",
        "    words = text.split()\n",
        "    filtered_words = [word for word in words if word not in lemmatized_stopwords]\n",
        "    return ' '.join(filtered_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a8137d6a-a8bd-4c32-ba0e-17c538335427",
      "metadata": {
        "id": "a8137d6a-a8bd-4c32-ba0e-17c538335427"
      },
      "outputs": [],
      "source": [
        "# Artikel einlesen\n",
        "data = pd.read_csv('artikel_tabelle.csv')\n",
        "data['länge_vorher'] = [len(i.split()) for i in data.Text]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lemmatisieren"
      ],
      "metadata": {
        "id": "j4V-1VJTkrkM"
      },
      "id": "j4V-1VJTkrkM"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6ca3297-ba81-4e7b-bd9e-5b422b3dff86",
      "metadata": {
        "collapsed": true,
        "id": "e6ca3297-ba81-4e7b-bd9e-5b422b3dff86"
      },
      "outputs": [],
      "source": [
        "# Lemmatisieren der Texte\n",
        "lemmatized = lemmatize_texts([data.Text[i] for i in range(data.shape[0])], 'de_core_news_lg')\n",
        "\n",
        "# Erstellen einer neuen Spalte mit den lemmatisierten Texten\n",
        "data['lemmatized_text'] = lemmatized\n",
        "\n",
        "# Ersetzen von Zeilenumbrüchen innerhalb von Wörtern\n",
        "data_lemmatized = data.replace('-\\n', '', regex=True)\n",
        "\n",
        "data_lemmatized[['Datum', 'länge_vorher', 'lemmatized_text']].to_csv('lemmatisierte_artikel.csv', index=False, encoding=\"utf-8\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Entfernen von nltk-Stoppwörtern"
      ],
      "metadata": {
        "id": "-OSkZAsxl0ts"
      },
      "id": "-OSkZAsxl0ts"
    },
    {
      "cell_type": "code",
      "source": [
        "data_lemmatized = pd.read_csv('lemmatisierte_artikel.csv')\n",
        "# Umlaute umschreiben\n",
        "data_lemmatized['lemmatized_text'] = convert_umlauts_strings(data_lemmatized['lemmatized_text'])\n",
        "# Spezielle Satzzeichen entfernen, einzelne Buchstaben und Zeichenketten mit mehr als 30 Buchstaben entfernen\n",
        "data_lemmatized['lemmatized_text'] = [' '.join(words) for words in sent_to_words(data_lemmatized['lemmatized_text'])]"
      ],
      "metadata": {
        "id": "4sAQXM5Sl3ue"
      },
      "id": "4sAQXM5Sl3ue",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data_lemmatized[['lemmatized_text']].head())"
      ],
      "metadata": {
        "id": "KDVzqjKOLzGN",
        "collapsed": true
      },
      "id": "KDVzqjKOLzGN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Standard-Stoppwörter von nltk entfernen\n",
        "data_lemmatized['text_preprocessed'] = remove_words(data_lemmatized['lemmatized_text'], convert_umlauts_strings(stop_words))\n",
        "\n",
        "# Wörter zählen\n",
        "gesamt_wörter_nachher = data_lemmatized['text_preprocessed'].apply(lambda x: len(str(x).split())).sum()\n",
        "gesamt_wörter_vorher = data_lemmatized['lemmatized_text'].apply(lambda x: len(str(x).split())).sum()\n",
        "\n",
        "# Ergebnisse ausgeben\n",
        "entfernte_wörter = gesamt_wörter_vorher - gesamt_wörter_nachher\n",
        "print(f\"Vor der Entfernung spezifischer Stoppwörter umfasste der Datensatz {gesamt_wörter_vorher} Wörter.\")\n",
        "print(f\"Durch die Entfernung entfielen {entfernte_wörter} Wörter.\")\n",
        "print(f\"Somit umfasst der Korpus jetzt noch {gesamt_wörter_nachher} Wörter.\")"
      ],
      "metadata": {
        "id": "felXyF0HoUo0"
      },
      "id": "felXyF0HoUo0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Speichern der bereinigten Texte."
      ],
      "metadata": {
        "id": "biaf4CkHonOo"
      },
      "id": "biaf4CkHonOo"
    },
    {
      "cell_type": "code",
      "source": [
        "text_length_lemmatized = [len(i.split()) for i in data_lemmatized.text_preprocessed] # Textlänge nach dem Bereinigen der Artikel\n",
        "\n",
        "data_lemmatized['länge_bereinigt'] = text_length_lemmatized\n",
        "\n",
        "# Datum in datetime Format umwandeln\n",
        "data_lemmatized['Datum'] = pd.to_datetime(data_lemmatized['Datum'], format='%d.%m.%Y')\n",
        "\n",
        "# Extrahieren von Jahr und Quartal aus dem Datum\n",
        "data_lemmatized['datum_jahr'] = data_lemmatized['Datum'].dt.year\n",
        "data_lemmatized['datum_quartal'] = data_lemmatized['Datum'].dt.quarter\n",
        "\n",
        "# Speichern in einer neuen .csv Datei\n",
        "data_lemmatized[['Datum', 'datum_jahr', 'datum_quartal', 'text_preprocessed', 'länge_vorher', 'länge_bereinigt']].to_csv(\n",
        "    'bereinigte_artikel.csv',\n",
        "    index=False\n",
        ")"
      ],
      "metadata": {
        "id": "kFInIaWYortx"
      },
      "id": "kFInIaWYortx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_lemmatized[['Datum', 'datum_jahr', 'datum_quartal', 'text_preprocessed', 'länge_vorher', 'länge_bereinigt']]"
      ],
      "metadata": {
        "id": "JmiUhqW3sOeW",
        "collapsed": true
      },
      "id": "JmiUhqW3sOeW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Entfernen spezifischer Stoppwörter\n",
        "Die Stoppwörter basieren auf der Liste von Latifi et al., (2024) und wurden manuell angepasst."
      ],
      "metadata": {
        "id": "Kwk0BC6OwImE"
      },
      "id": "Kwk0BC6OwImE"
    },
    {
      "cell_type": "code",
      "source": [
        "# Schritt 1: Einlesen der CSV mit spezifischen Stoppwörtern\n",
        "stopwords_df = pd.read_csv('self_filtered_stopwords_german_bundestag.csv', header=None)\n",
        "\n",
        "# Schritt 2: Extrahieren der lemmatisierten Stoppwörter aus der Tabelle\n",
        "lemmatized_stopwords = stopwords_df[2].drop(0).tolist()\n",
        "\n",
        "# Wörter vor der Entfernung spezifischer Stoppwörter zählen\n",
        "gesamt_wörter_vorher = data_lemmatized['text_preprocessed'].apply(lambda x: len(str(x).split())).sum()\n",
        "\n",
        "# Anwenden der Funktion auf die Spalte 'text_preprocessed'\n",
        "data_lemmatized['text_preprocessed_specific'] = data_lemmatized['text_preprocessed'].apply(remove_specific_stopwords)\n",
        "\n",
        "# Wörter nach der Entfernung spezifischer Stoppwörter zählen\n",
        "gesamt_wörter_nachher = data_lemmatized['text_preprocessed_specific'].apply(lambda x: len(str(x).split())).sum()\n",
        "\n",
        "# Schritt 6: Ergebnisse ausgeben\n",
        "entfernte_wörter = gesamt_wörter_vorher - gesamt_wörter_nachher\n",
        "print(f\"Vor der Entfernung spezifischer Stoppwörter umfasste der Datensatz {gesamt_wörter_vorher} Wörter.\")\n",
        "print(f\"Durch die Entfernung entfielen {entfernte_wörter} Wörter.\")\n",
        "print(f\"Somit umfasst der Korpus jetzt noch {gesamt_wörter_nachher} Wörter.\")"
      ],
      "metadata": {
        "id": "XHalxO_8wPvm"
      },
      "id": "XHalxO_8wPvm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_length_specific = [len(i.split()) for i in data_lemmatized.text_preprocessed_specific] # Textlänge nach dem Bereinigen der Artikel\n",
        "\n",
        "data_lemmatized['länge_bereinigt_spezifisch'] = text_length_specific\n",
        "\n",
        "# Speichern in einer neuen .csv Datei\n",
        "data_lemmatized[['Datum', 'datum_jahr', 'datum_quartal', 'text_preprocessed_specific', 'länge_vorher', 'länge_bereinigt', 'länge_bereinigt_spezifisch']].to_csv(\n",
        "    'bereinigte_artikel_spezifisch.csv',\n",
        "    index=False\n",
        ")"
      ],
      "metadata": {
        "id": "5mZ7AE2b0rxC"
      },
      "id": "5mZ7AE2b0rxC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Graphische Darstellung"
      ],
      "metadata": {
        "id": "fvRE0BIe1cU8"
      },
      "id": "fvRE0BIe1cU8"
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "lt9CGU5K4sWz"
      },
      "id": "lt9CGU5K4sWz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_lemmatized = pd.read_csv('bereinigte_artikel_spezifisch.csv')\n",
        "\n",
        "# Definieren von Bins für Gruppen von 20 Tokens\n",
        "bin_edges = range(0, data_lemmatized[\"länge_vorher\"].max() + 20, 20)\n",
        "\n",
        "# Frequenzberechnung für jede Textlängenspalte\n",
        "freq_original = pd.cut(data_lemmatized[\"länge_vorher\"], bins=bin_edges).value_counts().sort_index()\n",
        "freq_nltk = pd.cut(data_lemmatized[\"länge_bereinigt\"], bins=bin_edges).value_counts().sort_index()\n",
        "freq_specific = pd.cut(data_lemmatized[\"länge_bereinigt_spezifisch\"], bins=bin_edges).value_counts().sort_index()\n",
        "\n",
        "# Mittelpunkte der Bins\n",
        "bin_midpoints = [interval.mid for interval in freq_original.index.categories]\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.bar(bin_midpoints, freq_original, width=18, label=\"Originale Artikellänge\", alpha=0.7, color='blue')\n",
        "plt.bar(bin_midpoints, freq_nltk, width=18, label=\"Länge nltk-Stoppwörter entfernt\", alpha=0.7, color='orange')\n",
        "plt.bar(bin_midpoints, freq_specific, width=18, label=\"Länge spezifische Stoppwörter entfernt\", alpha=0.7, color='green')\n",
        "\n",
        "# Achsenbeschriftungen und Titel\n",
        "plt.xlabel(\"Anzahl der Wörter im Artikel\", fontsize=12)\n",
        "plt.ylabel(\"Anzahl der Artikel\", fontsize=12)\n",
        "# plt.title(\"\", fontsize=14)\n",
        "plt.legend()\n",
        "plt.grid(alpha=0.3)\n",
        "# Begrenze die x-Achse auf 4000 Wörter\n",
        "plt.xlim(0, 4000)\n",
        "plt.tight_layout()\n",
        "\n",
        "# Speichern der Grafik\n",
        "plt.savefig('EDA_artikel_1.png', format='png', dpi=600)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "VdfgFd311ga1",
        "collapsed": true
      },
      "id": "VdfgFd311ga1",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python (ba_env)",
      "language": "python",
      "name": "ba_env"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.10"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}