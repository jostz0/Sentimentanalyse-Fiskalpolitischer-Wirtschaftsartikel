{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP4ue6iEtpnE5leTRuEoQuc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jostz0/Sentimentanalyse-Fiskalpolitischer-Wirtschaftsartikel/blob/main/scoring_modell.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download de_core_news_md"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "wN-XTfqMzwYM",
        "outputId": "8f988695-21b3-43d9-9ea4-6d3b4fbfc7dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting de-core-news-md==3.7.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_md-3.7.0/de_core_news_md-3.7.0-py3-none-any.whl (44.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 MB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from de-core-news-md==3.7.0) (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-md==3.7.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-md==3.7.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-md==3.7.0) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-md==3.7.0) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-md==3.7.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-md==3.7.0) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-md==3.7.0) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-md==3.7.0) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-md==3.7.0) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-md==3.7.0) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-md==3.7.0) (0.12.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-md==3.7.0) (4.66.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-md==3.7.0) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-md==3.7.0) (2.9.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-md==3.7.0) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-md==3.7.0) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-md==3.7.0) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-md==3.7.0) (3.4.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-md==3.7.0) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->de-core-news-md==3.7.0) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->de-core-news-md==3.7.0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->de-core-news-md==3.7.0) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->de-core-news-md==3.7.0) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-md==3.7.0) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-md==3.7.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-md==3.7.0) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-md==3.7.0) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->de-core-news-md==3.7.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->de-core-news-md==3.7.0) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->de-core-news-md==3.7.0) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->de-core-news-md==3.7.0) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->de-core-news-md==3.7.0) (13.9.3)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->de-core-news-md==3.7.0) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->de-core-news-md==3.7.0) (7.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.0->de-core-news-md==3.7.0) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->de-core-news-md==3.7.0) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->de-core-news-md==3.7.0) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->de-core-news-md==3.7.0) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->de-core-news-md==3.7.0) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->de-core-news-md==3.7.0) (0.1.2)\n",
            "Installing collected packages: de-core-news-md\n",
            "Successfully installed de-core-news-md-3.7.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('de_core_news_md')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UbLEs61v_rjS",
        "outputId": "8d458ee1-6de5-49fb-dd65-88c979eacc73"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Anzahl der expansiven Begriffe: 123\n",
            "Anzahl der kontraktiven Begriffe: 97\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import spacy\n",
        "\n",
        "# Laden des vortrainierten spaCy-Modells (für Deutsch)\n",
        "nlp = spacy.load('de_core_news_md')\n",
        "\n",
        "# Einlesen der expansiven und kontraktiven Begriffe\n",
        "expansionary_terms = pd.read_csv('expansionary_terms_preprocessed.csv', header=None)[0].tolist()\n",
        "contractionary_terms = pd.read_csv('contractionary_terms_preprocessed.csv', header=None)[0].tolist()\n",
        "\n",
        "print(\"Anzahl der expansiven Begriffe:\", len(expansionary_terms))\n",
        "print(\"Anzahl der kontraktiven Begriffe:\", len(contractionary_terms))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Alternative: Diesen Code nur ausführen, wenn die bestehenden Listen um Synonyme erweitert werden sollen!**"
      ],
      "metadata": {
        "id": "ai-vgI883CIu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Funktion zur Ermittlung ähnlicher Begriffe und Erstellung einer Zuordnungstabelle\n",
        "def find_similar_terms_mapping(term_list, n=5):\n",
        "    term_mapping = {}\n",
        "    for term in term_list:\n",
        "        token = nlp(term)\n",
        "        if token.has_vector and token.vector_norm > 0:  # Überprüfen, ob der Originalbegriff einen Vektor hat\n",
        "            similar_terms = set()\n",
        "            for word in nlp.vocab:\n",
        "                # Überprüfen, ob das Wort einen gültigen Vektor hat und nicht denselben Begriff repräsentiert\n",
        "                if word.is_alpha and word.has_vector and word.vector_norm > 0 and word.text != term:\n",
        "                    similarity = token.similarity(word)\n",
        "                    if similarity > 0.7:  # Schwellenwert für Ähnlichkeit\n",
        "                        similar_terms.add(word.text.lower())\n",
        "            term_mapping[term] = list(similar_terms)[:n]  # Maximal n Synonyme aufnehmen\n",
        "    return term_mapping\n",
        "\n",
        "# Funktion zum Entfernen von Synonymen, die Wörter aus den Originalbegriffen enthalten\n",
        "def remove_duplicate_synonyms(df):\n",
        "    for i, row in df.iterrows():\n",
        "        original_terms = set(row['Original Term'].split())\n",
        "        for col in df.columns[1:]:\n",
        "            if pd.notna(row[col]):  # Prüfen, ob das Synonym vorhanden ist\n",
        "                synonym_words = set(row[col].split())\n",
        "                # Entfernen, wenn das Synonym ein Wort enthält, das im Originalterm vorkommt\n",
        "                if original_terms & synonym_words:\n",
        "                    df.at[i, col] = None\n",
        "    return df\n",
        "\n",
        "# Erstellen der Zuordnungstabellen für die expansiven und kontraktiven Begriffe\n",
        "expanded_expansionary_mapping = find_similar_terms_mapping(expansionary_terms)\n",
        "expanded_contractionary_mapping = find_similar_terms_mapping(contractionary_terms)\n",
        "\n",
        "# Konvertieren der Zuordnungen in DataFrames\n",
        "expansionary_df = pd.DataFrame.from_dict(expanded_expansionary_mapping, orient='index').reset_index()\n",
        "expansionary_df.columns = ['Original Term'] + [f'Synonym {i+1}' for i in range(expansionary_df.shape[1] - 1)]\n",
        "\n",
        "contractionary_df = pd.DataFrame.from_dict(expanded_contractionary_mapping, orient='index').reset_index()\n",
        "contractionary_df.columns = ['Original Term'] + [f'Synonym {i+1}' for i in range(contractionary_df.shape[1] - 1)]\n",
        "\n",
        "# Anwenden der Funktion zur Bereinigung von doppelten Synonymen\n",
        "expansionary_df = remove_duplicate_synonyms(expansionary_df)\n",
        "contractionary_df = remove_duplicate_synonyms(contractionary_df)\n",
        "\n",
        "# Extrahieren der erweiterten Listen für das Scoring\n",
        "expanded_expansionary_terms = set(expansionary_terms)\n",
        "for synonyms in expansionary_df.iloc[:, 1:].values.flatten():\n",
        "    if pd.notna(synonyms):\n",
        "        expanded_expansionary_terms.add(synonyms)\n",
        "\n",
        "expanded_contractionary_terms = set(contractionary_terms)\n",
        "for synonyms in contractionary_df.iloc[:, 1:].values.flatten():\n",
        "    if pd.notna(synonyms):\n",
        "        expanded_contractionary_terms.add(synonyms)\n",
        "\n",
        "# Speichern der DataFrames in CSV-Dateien\n",
        "expansionary_df.to_csv('expanded_expansionary_terms.csv', index=False, encoding='utf-8')\n",
        "contractionary_df.to_csv('expanded_contractionary_terms.csv', index=False, encoding='utf-8')\n",
        "\n",
        "print(f\"Anzahl der erweiterten expansiven Begriffe: {len(expanded_expansionary_terms)}\")\n",
        "print(f\"Anzahl der erweiterten kontraktiven Begriffe: {len(expanded_contractionary_terms)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rGjz9bn9z5EK",
        "outputId": "bd989eb8-6aa1-473c-ec03-737ad9dfc603"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Anzahl der erweiterten expansiven Begriffe: 133\n",
            "Anzahl der erweiterten kontraktiven Begriffe: 105\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dieser Code liest die vorher bereinigten Artikel ein und teilt den Text in die einzelnen Artikel auf. Außerdem extrahiert er das Erscheinungsdatum des Artikels."
      ],
      "metadata": {
        "id": "zCX0Oyv63Ma2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lesen und Segmentieren der Artikel\n",
        "with open('bereinigt_spezifisch.txt', 'r', encoding='utf-8') as file:\n",
        "    text = file.read()\n",
        "\n",
        "articles = text.split('handelsblatt prinen')[1:]  # Segmentierung ab dem ersten Kennzeichen\n",
        "articles = ['handelsblatt prinen' + article for article in articles]\n",
        "\n",
        "print(\"Anzahl der Artikel:\", len(articles))\n",
        "\n",
        "# Extrahieren des Datums aus den Artikeln\n",
        "def extract_date(article):\n",
        "    # Angenommen, das Datum steht direkt nach \"handelsblatt prinen \"\n",
        "    date_marker = article.split()[2]\n",
        "    # Formatierung in DD.MM.YYYY\n",
        "    if len(date_marker) == 8:  # Überprüfung, ob das Datum in der erwarteten Form ist\n",
        "        formatted_date = f\"{date_marker[:2]}.{date_marker[2:4]}.{date_marker[4:]}\"\n",
        "        return formatted_date"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UD51hfHD1Rrm",
        "outputId": "aa2de8d5-40f1-4c3b-92ab-372527d8f209"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Anzahl der Artikel: 137\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hier bestimmen wir den Score für jeden Artikel und erstellen eine Tabelle, in der wir die Scores und zugehörigen Artikel auslesen können.\n",
        "\n",
        "**Ohne Verwendung von Synonymen!**"
      ],
      "metadata": {
        "id": "oE9Q5yOe2nBa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Funktion zum Zählen der Begriffe in einem Text\n",
        "def count_terms(text, term_list):\n",
        "    words = text.split()\n",
        "    return sum(1 for word in words if word in term_list)\n",
        "\n",
        "article_scores = []\n",
        "for article in articles:\n",
        "    date = extract_date(article)\n",
        "    expansive_count = count_terms(article, expansionary_terms)\n",
        "    contractionary_count = count_terms(article, contractionary_terms)\n",
        "    score = expansive_count + contractionary_count\n",
        "    normalized_score = score / len(article.split()) if len(article.split()) > 0 else 0  # Normalisierung des Scores\n",
        "    article_scores.append({\n",
        "        'Datum': date,\n",
        "        'Vorschau': article[:100],\n",
        "        'Expansive Begriffe': expansive_count,\n",
        "        'Kontraktive Begriffe': contractionary_count,\n",
        "        'Score': normalized_score\n",
        "    })\n",
        "\n",
        "# Beispielhafte Ausgabe der Scores\n",
        "for i, article_data in enumerate(article_scores[:5]):  # Zeige die ersten 5 Artikel\n",
        "    print(f\"Artikel {i+1}:\")\n",
        "    print(f\"Vorschau: {article_data['Vorschau']}...\")\n",
        "    print(f\"Expansive Begriffe: {article_data['Expansive Begriffe']}, Kontraktive Begriffe: {article_data['Kontraktive Begriffe']}, Score: {article_data['Score']}\\n\")\n",
        "\n",
        "# Erstellen eines DataFrames für die Ausgabe\n",
        "scores_df = pd.DataFrame(article_scores)\n",
        "\n",
        "# Speichern der Ergebnisse in einer CSV-Datei\n",
        "scores_df.to_csv('artikel_scores.csv', index=False, encoding='utf-8')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "clPhaC6X1lAQ",
        "outputId": "70f5b91d-e765-460b-afcc-a53d71a57e84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Artikel 1:\n",
            "Vorschau: handelsblatt prinen 28102022 s 10 steuerschätzung rekordeinnahme krise endgueltig bestaetigen handel...\n",
            "Expansive Begriffe: 5, Kontraktive Begriffe: 6, Score: 0.03333333333333333\n",
            "\n",
            "Artikel 2:\n",
            "Vorschau: handelsblatt prinen 17042023 s 4 special gita gopinath groebsten usspitzenoekonomin finanzsystem rue...\n",
            "Expansive Begriffe: 9, Kontraktive Begriffe: 2, Score: 0.022821576763485476\n",
            "\n",
            "Artikel 3:\n",
            "Vorschau: handelsblatt prinen 16022023 s 7 des stabilitätspakts lindner europaeisch schuldenregel retten suede...\n",
            "Expansive Begriffe: 0, Kontraktive Begriffe: 13, Score: 0.04609929078014184\n",
            "\n",
            "Artikel 4:\n",
            "Vorschau: handelsblatt prinen 24062022 s 58 wochenende valdis dombrovskis periode billig geld entgegen taumeln...\n",
            "Expansive Begriffe: 3, Kontraktive Begriffe: 5, Score: 0.02056555269922879\n",
            "\n",
            "Artikel 5:\n",
            "Vorschau: handelsblatt prinen 17062022 s 3 special eurokrise ezbkrisensitzung erster europaeisch finanzministe...\n",
            "Expansive Begriffe: 0, Kontraktive Begriffe: 3, Score: 0.005309734513274336\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Alternative: Diesen Code nur ausführen, wenn die expansiven und kontraktiven Begriffe um Synonyme erweitert wurden!**"
      ],
      "metadata": {
        "id": "rq4eChLI2of3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Funktion zum Zählen der Begriffe in einem Text\n",
        "def count_terms(text, term_list):\n",
        "    words = text.split()\n",
        "    return sum(1 for word in words if word in term_list), len(words)\n",
        "\n",
        "# Berechnung der Scores für die Artikel\n",
        "article_scores_sy = []\n",
        "for article in articles:\n",
        "    date = extract_date(article)\n",
        "    expansive_count_sy, total_words = count_terms(article, expanded_expansionary_terms)\n",
        "    contractionary_count_sy, _ = count_terms(article, expanded_contractionary_terms)\n",
        "    score_sy = expansive_count_sy + contractionary_count_sy\n",
        "    normalized_score_sy = score_sy / total_words if total_words > 0 else 0  # Vermeidung von Division durch Null\n",
        "    article_scores_sy.append({\n",
        "        'Datum': date,\n",
        "        'Vorschau': article[:100],\n",
        "        'Expansive Begriffe': expansive_count_sy,\n",
        "        'Kontraktive Begriffe': contractionary_count_sy,\n",
        "        'Score': normalized_score_sy\n",
        "    })\n",
        "\n",
        "# Beispielhafte Ausgabe der Scores\n",
        "for i, article_data in enumerate(article_scores_sy[:5]):  # Zeige die ersten 5 Artikel\n",
        "    print(f\"Artikel {i+1}:\")\n",
        "    print(f\"Vorschau: {article_data['Vorschau']}...\")\n",
        "    print(f\"Expansive Begriffe: {article_data['Expansive Begriffe']}, Kontraktive Begriffe: {article_data['Kontraktive Begriffe']}, Score: {article_data['Score']}\\n\")\n",
        "\n",
        "# Erstellen eines DataFrames für die Ausgabe\n",
        "scores_sy_df = pd.DataFrame(article_scores_sy)\n",
        "\n",
        "# Speichern der Ergebnisse in einer CSV-Datei\n",
        "scores_sy_df.to_csv('artikel_scores_sy.csv', index=False, encoding='utf-8')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K0CzGrKW1dah",
        "outputId": "c6fd83d7-dfa1-4b40-d48c-0454bc40fe24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Artikel 1:\n",
            "Vorschau: handelsblatt prinen 28102022 s 10 steuerschätzung rekordeinnahme krise endgueltig bestaetigen handel...\n",
            "Expansive Begriffe: 8, Kontraktive Begriffe: 9, Score: 0.051515151515151514\n",
            "\n",
            "Artikel 2:\n",
            "Vorschau: handelsblatt prinen 17042023 s 4 special gita gopinath groebsten usspitzenoekonomin finanzsystem rue...\n",
            "Expansive Begriffe: 15, Kontraktive Begriffe: 8, Score: 0.04771784232365145\n",
            "\n",
            "Artikel 3:\n",
            "Vorschau: handelsblatt prinen 16022023 s 7 des stabilitätspakts lindner europaeisch schuldenregel retten suede...\n",
            "Expansive Begriffe: 4, Kontraktive Begriffe: 13, Score: 0.06028368794326241\n",
            "\n",
            "Artikel 4:\n",
            "Vorschau: handelsblatt prinen 24062022 s 58 wochenende valdis dombrovskis periode billig geld entgegen taumeln...\n",
            "Expansive Begriffe: 7, Kontraktive Begriffe: 7, Score: 0.03598971722365039\n",
            "\n",
            "Artikel 5:\n",
            "Vorschau: handelsblatt prinen 17062022 s 3 special eurokrise ezbkrisensitzung erster europaeisch finanzministe...\n",
            "Expansive Begriffe: 3, Kontraktive Begriffe: 4, Score: 0.012389380530973451\n",
            "\n"
          ]
        }
      ]
    }
  ]
}